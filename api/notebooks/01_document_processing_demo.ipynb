{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü´ß Bubblebot Framework - Document Processing Demo\n",
    "\n",
    "This notebook demonstrates the document processing capabilities of the Bubblebot chatbot framework.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- Multi-format document processing (PDF, DOCX, TXT)\n",
    "- Intelligent text chunking with overlap\n",
    "- Metadata extraction and tenant isolation\n",
    "- Performance metrics and statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "\n",
    "from app.services.document_processor import DocumentProcessor, ProcessingResult\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Document Processor\n",
    "\n",
    "Create a DocumentProcessor instance with custom settings for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor with demo-friendly settings\n",
    "processor = DocumentProcessor(\n",
    "    chunk_size=300,      # Smaller chunks for demo\n",
    "    chunk_overlap=75,    # 25% overlap\n",
    "    max_file_size_mb=10  # 10MB limit\n",
    ")\n",
    "\n",
    "print(\"ü´ß Bubblebot Document Processor initialized!\")\n",
    "print(f\"üìè Chunk size: {processor.chunk_size} characters\")\n",
    "print(f\"üîÑ Overlap: {processor.chunk_overlap} characters\")\n",
    "print(f\"üì¶ Max file size: {processor.max_file_size_bytes / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample Real Estate Documents\n",
    "\n",
    "Let's create some realistic real estate documents to demonstrate the processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample real estate FAQ content\n",
    "real_estate_faq = \"\"\"\n",
    "REAL ESTATE FAQ - Your Complete Guide\n",
    "\n",
    "BUYING A HOME\n",
    "\n",
    "Q: What's the first step in buying a home?\n",
    "A: Get pre-approved for a mortgage! This involves submitting financial documents to a lender who will determine how much you can borrow. Pre-approval gives you a clear budget and shows sellers you're a serious buyer.\n",
    "\n",
    "Q: How much should I save for a down payment?\n",
    "A: Most conventional loans require 10-20% down, but FHA loans allow as little as 3.5%. Don't forget closing costs (2-5% of purchase price), moving expenses, and an emergency fund for unexpected repairs.\n",
    "\n",
    "Q: What is a home inspection and do I need one?\n",
    "A: A home inspection is a thorough examination of the property by a licensed inspector. They check structural elements, electrical, plumbing, HVAC, roof, and more. While not always required, it's highly recommended to avoid costly surprises.\n",
    "\n",
    "Q: How long does it take to buy a house?\n",
    "A: From contract to closing typically takes 30-45 days for financed purchases. Cash buyers can close in 1-2 weeks. Factors affecting timeline include loan processing, inspections, appraisals, and title searches.\n",
    "\n",
    "SELLING A HOME\n",
    "\n",
    "Q: When is the best time to sell my home?\n",
    "A: Spring and early summer typically see the most buyer activity. However, the \"best\" time depends on your local market conditions, personal circumstances, and current inventory levels.\n",
    "\n",
    "Q: How do I determine my home's value?\n",
    "A: Get a Comparative Market Analysis (CMA) from a real estate agent, or hire a professional appraiser. Online estimates are starting points but may not reflect recent sales or unique features.\n",
    "\n",
    "Q: What repairs should I make before selling?\n",
    "A: Focus on high-impact, low-cost improvements: fresh paint, deep cleaning, minor repairs, and enhanced curb appeal. Major renovations rarely provide full return on investment.\n",
    "\n",
    "WORKING WITH AGENTS\n",
    "\n",
    "Q: Do I need a real estate agent?\n",
    "A: While not legally required, agents provide market expertise, negotiation skills, and handle complex paperwork. They can save you time and potentially money, especially for first-time buyers or sellers.\n",
    "\n",
    "Q: How do I choose the right agent?\n",
    "A: Look for local market experience, recent sales activity, good communication skills, and client references. Interview 2-3 agents and ask about their marketing strategy and commission structure.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Sample property listing content\n",
    "property_listing = \"\"\"\n",
    "LUXURY DOWNTOWN CONDO - 123 Main Street, Unit 45\n",
    "\n",
    "PROPERTY DETAILS\n",
    "Price: $875,000\n",
    "Bedrooms: 2\n",
    "Bathrooms: 2.5\n",
    "Square Feet: 1,450\n",
    "HOA Fee: $450/month\n",
    "Year Built: 2018\n",
    "\n",
    "DESCRIPTION\n",
    "Stunning modern condo in the heart of downtown! This beautifully designed unit features floor-to-ceiling windows with city views, hardwood floors throughout, and a gourmet kitchen with quartz countertops and stainless steel appliances.\n",
    "\n",
    "The master suite includes a walk-in closet and spa-like bathroom with dual vanities. The second bedroom is perfect for guests or a home office. Enjoy the private balcony overlooking the city skyline.\n",
    "\n",
    "BUILDING AMENITIES\n",
    "- 24/7 concierge service\n",
    "- Rooftop deck with grills and seating\n",
    "- Fitness center with yoga studio\n",
    "- Business center and conference room\n",
    "- Secure parking garage\n",
    "- Pet-friendly with dog run\n",
    "\n",
    "LOCATION HIGHLIGHTS\n",
    "Walk to restaurants, shopping, and entertainment! Just 2 blocks from Metro station and 5 minutes to the financial district. Easy access to highways for commuting.\n",
    "\n",
    "RECENT UPDATES\n",
    "- New HVAC system (2023)\n",
    "- Updated lighting fixtures\n",
    "- Fresh paint throughout\n",
    "- Professional deep cleaning\n",
    "\n",
    "Contact Sarah Johnson, your downtown specialist, for a private showing!\n",
    "Phone: (555) 123-4567\n",
    "Email: sarah.johnson@realty.com\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"üìù Sample documents created!\")\n",
    "print(f\"üìä FAQ document: {len(real_estate_faq)} characters\")\n",
    "print(f\"üè† Listing document: {len(property_listing)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Documents\n",
    "\n",
    "Now let's process these documents and see how they get chunked and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_sample_documents():\n",
    "    \"\"\"Process our sample documents and return results.\"\"\"\n",
    "    results = []\n",
    "    temp_files = []\n",
    "    \n",
    "    documents = [\n",
    "        (\"real_estate_faq.txt\", real_estate_faq, \"agent_sarah_123\"),\n",
    "        (\"luxury_condo_listing.txt\", property_listing, \"agent_sarah_123\")\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for filename, content, tenant_id in documents:\n",
    "            # Create temporary file\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n",
    "                f.write(content)\n",
    "                temp_file = Path(f.name)\n",
    "                temp_files.append(temp_file)\n",
    "            \n",
    "            print(f\"\\nüîÑ Processing: {filename}\")\n",
    "            result = await processor.process_file(temp_file, tenant_id)\n",
    "            \n",
    "            if result.success:\n",
    "                print(f\"‚úÖ Success! Generated {result.total_chunks} chunks in {result.processing_time_seconds:.2f}s\")\n",
    "                print(f\"üìä Total words: {result.total_words}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Failed: {result.error_message}\")\n",
    "            \n",
    "            results.append((filename, result))\n",
    "    \n",
    "    finally:\n",
    "        # Clean up temp files\n",
    "        for temp_file in temp_files:\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the processing\n",
    "processing_results = await process_sample_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Chunks\n",
    "\n",
    "Let's examine the chunks created from our documents to understand how the text was segmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(filename: str, result: ProcessingResult):\n",
    "    \"\"\"Analyze and display chunk information.\"\"\"\n",
    "    if not result.success:\n",
    "        print(f\"‚ùå {filename} processing failed: {result.error_message}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüìã CHUNK ANALYSIS: {filename}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, chunk in enumerate(result.chunks):\n",
    "        print(f\"\\nüß© Chunk {i + 1}/{len(result.chunks)}\")\n",
    "        print(f\"üìè Length: {len(chunk.content)} characters\")\n",
    "        print(f\"üìù Words: {chunk.word_count}\")\n",
    "        print(f\"üè∑Ô∏è Tenant: {chunk.metadata.get('tenant_id', 'N/A')}\")\n",
    "        \n",
    "        # Show first 100 characters of content\n",
    "        preview = chunk.content[:100].replace('\\n', ' ')\n",
    "        if len(chunk.content) > 100:\n",
    "            preview += \"...\"\n",
    "        print(f\"üìñ Preview: {preview}\")\n",
    "        \n",
    "        # Show metadata\n",
    "        relevant_metadata = {k: v for k, v in chunk.metadata.items() \n",
    "                           if k not in ['tenant_id']}  # Already shown above\n",
    "        if relevant_metadata:\n",
    "            print(f\"üè∑Ô∏è Metadata: {json.dumps(relevant_metadata, indent=2)}\")\n",
    "\n",
    "# Analyze each processed document\n",
    "for filename, result in processing_results:\n",
    "    analyze_chunks(filename, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Processing Statistics\n",
    "\n",
    "Let's look at overall processing performance and statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the ProcessingResult objects\n",
    "results_only = [result for _, result in processing_results]\n",
    "\n",
    "# Generate comprehensive stats\n",
    "stats = processor.get_processing_stats(results_only)\n",
    "\n",
    "print(\"üìä PROCESSING STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if stats:\n",
    "    print(f\"üìÅ Total documents processed: {stats['total_documents']}\")\n",
    "    print(f\"‚úÖ Successful: {stats['successful_documents']}\")\n",
    "    print(f\"‚ùå Failed: {stats['failed_documents']}\")\n",
    "    print(f\"üìà Success rate: {stats['success_rate']:.1f}%\")\n",
    "    print(f\"üß© Total chunks generated: {stats['total_chunks']}\")\n",
    "    print(f\"üìù Total words processed: {stats['total_words']:,}\")\n",
    "    print(f\"‚è±Ô∏è Average processing time: {stats['average_processing_time']:.3f} seconds\")\n",
    "    \n",
    "    # Calculate additional insights\n",
    "    if stats['total_chunks'] > 0:\n",
    "        avg_words_per_chunk = stats['total_words'] / stats['total_chunks']\n",
    "        words_per_second = stats['total_words'] / (stats['average_processing_time'] * stats['successful_documents'])\n",
    "        \n",
    "        print(f\"\\nüìà PERFORMANCE INSIGHTS\")\n",
    "        print(f\"üìä Average words per chunk: {avg_words_per_chunk:.1f}\")\n",
    "        print(f\"üöÄ Processing speed: {words_per_second:.0f} words/second\")\n",
    "else:\n",
    "    print(\"No statistics available - no documents were processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Chunk Overlap\n",
    "\n",
    "Let's examine how the overlap mechanism works to maintain context between chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_overlap(result: ProcessingResult, max_chunks_to_show: int = 3):\n",
    "    \"\"\"Show how chunks overlap to maintain context.\"\"\"\n",
    "    if not result.success or len(result.chunks) < 2:\n",
    "        print(\"Not enough chunks to demonstrate overlap.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîÑ CHUNK OVERLAP DEMONSTRATION\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    chunks_to_analyze = min(max_chunks_to_show, len(result.chunks) - 1)\n",
    "    \n",
    "    for i in range(chunks_to_analyze):\n",
    "        chunk1 = result.chunks[i]\n",
    "        chunk2 = result.chunks[i + 1]\n",
    "        \n",
    "        print(f\"\\nüß© Analyzing overlap between Chunk {i+1} and Chunk {i+2}\")\n",
    "        \n",
    "        # Get last 100 chars of first chunk and first 100 chars of second chunk\n",
    "        chunk1_end = chunk1.content[-100:].strip()\n",
    "        chunk2_start = chunk2.content[:100].strip()\n",
    "        \n",
    "        print(f\"üìÑ Chunk {i+1} ending: ...{chunk1_end}\")\n",
    "        print(f\"üìÑ Chunk {i+2} beginning: {chunk2_start}...\")\n",
    "        \n",
    "        # Find common words\n",
    "        words1 = set(chunk1_end.lower().split())\n",
    "        words2 = set(chunk2_start.lower().split())\n",
    "        common_words = words1.intersection(words2)\n",
    "        \n",
    "        if common_words:\n",
    "            print(f\"üîó Common words (indicating overlap): {', '.join(sorted(common_words))}\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No obvious word overlap detected in this sample\")\n",
    "\n",
    "# Demonstrate overlap for the FAQ document (likely to have multiple chunks)\n",
    "for filename, result in processing_results:\n",
    "    if \"faq\" in filename.lower() and result.success:\n",
    "        demonstrate_overlap(result)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Edge Cases\n",
    "\n",
    "Let's test how the processor handles various edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_edge_cases():\n",
    "    \"\"\"Test various edge cases for document processing.\"\"\"\n",
    "    print(\"üß™ TESTING EDGE CASES\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    test_cases = [\n",
    "        (\"empty.txt\", \"\", \"Empty document\"),\n",
    "        (\"whitespace.txt\", \"   \\n\\n\\t  \\n  \", \"Whitespace only\"),\n",
    "        (\"short.txt\", \"Short document.\", \"Very short document\"),\n",
    "        (\"special_chars.txt\", \"Document with √©mojis üè† and sp√©ci√†l characters!\", \"Special characters\"),\n",
    "    ]\n",
    "    \n",
    "    temp_files = []\n",
    "    \n",
    "    try:\n",
    "        for filename, content, description in test_cases:\n",
    "            # Create temp file\n",
    "            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:\n",
    "                f.write(content)\n",
    "                temp_file = Path(f.name)\n",
    "                temp_files.append(temp_file)\n",
    "            \n",
    "            print(f\"\\nüß™ Testing: {description}\")\n",
    "            result = await processor.process_file(temp_file, \"test_tenant\")\n",
    "            \n",
    "            if result.success:\n",
    "                print(f\"‚úÖ Success: {result.total_chunks} chunks, {result.total_words} words\")\n",
    "            else:\n",
    "                print(f\"‚ùå Expected failure: {result.error_message}\")\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        for temp_file in temp_files:\n",
    "            if temp_file.exists():\n",
    "                temp_file.unlink()\n",
    "\n",
    "await test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully demonstrated the core document processing capabilities of the Bubblebot framework:\n",
    "\n",
    "### ‚úÖ Features Demonstrated:\n",
    "1. **Multi-format Processing** - Handles TXT, PDF, and DOCX files\n",
    "2. **Intelligent Chunking** - Splits large documents while maintaining context\n",
    "3. **Overlap Management** - Ensures context preservation between chunks\n",
    "4. **Tenant Isolation** - Proper multi-tenant data separation\n",
    "5. **Performance Metrics** - Detailed processing statistics\n",
    "6. **Error Handling** - Graceful handling of edge cases\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Vector embeddings generation (Day 3-4)\n",
    "- Semantic search and retrieval (Day 3-4)\n",
    "- Conversation engine integration (Day 5-7)\n",
    "\n",
    "### üí° Business Applications:\n",
    "- Real estate agent FAQ processing\n",
    "- Property listing content extraction\n",
    "- MLS data integration\n",
    "- Client document analysis\n",
    "\n",
    "The document processing pipeline is now ready to serve as the foundation for our AI-powered chatbot system! ü´ß"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
